{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Timer\n",
    "from sklearn.model_selection import KFold\n",
    "from lion_pytorch import Lion\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from utils.utils import MoleculeDataModule, evaluate_model\n",
    "from utils.train import MoleculeModel\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_scatter import scatter_mean\n",
    "import torch.nn.functional as F\n",
    "from utils.efficient_kan import KAN, KANLinear\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "def initialize_cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "        print(\"cuda\", torch.cuda.is_available())\n",
    "        print(torch.cuda.get_device_name(0))\n",
    "        torch.cuda.empty_cache()\n",
    "    else:\n",
    "        print(\"CUDA is not available.\")\n",
    "\n",
    "initialize_cuda()\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pytorch_lightning.trainer.connectors.data_connector\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"lightning_fabric.plugins.environments.slurm\")\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from torch_geometric.data import DataLoader as GeoDataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model_full(model, dataset, batch_size):\n",
    "    dataloader = GeoDataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=32)\n",
    "    model.eval()\n",
    "    all_pred, all_true = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch = batch.to(model.device)\n",
    "            y_hat = model(batch.x, batch.edge_index, batch.edge_attr)\n",
    "            all_pred.extend(y_hat.cpu().numpy())\n",
    "            all_true.extend(batch.y.cpu().numpy())\n",
    "\n",
    "    all_pred, all_true = np.array(all_pred), np.array(all_true)\n",
    "    rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
    "    r2 = r2_score(all_true, all_pred)\n",
    "\n",
    "    print(f'Total RMSE: {rmse:.4f}')\n",
    "    print(f'Total R²: {r2:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomEdgeInteraction(nn.Module):\n",
    "    def __init__(self, in_features, edge_features, out_features, edge_importance=1.0, dropout_rate=0.1, use_batch_norm=True):\n",
    "        super(AtomEdgeInteraction, self).__init__()\n",
    "        self.edge_importance = edge_importance\n",
    "        self.interaction = KANLinear(in_features + edge_features, out_features)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.batch_norm = nn.BatchNorm1d(out_features) if use_batch_norm else nn.Identity()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.residual = nn.Linear(in_features, out_features) if in_features != out_features else nn.Identity()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        edge_features = edge_attr * self.edge_importance\n",
    "        atom_features = x[row]\n",
    "        combined_features = torch.cat([atom_features, edge_features], dim=-1)\n",
    "        updated_features = self.interaction(combined_features)\n",
    "        updated_features = self.activation(updated_features)\n",
    "        updated_features = self.batch_norm(updated_features)\n",
    "        updated_features = self.dropout(updated_features)\n",
    "        residual_features = self.residual(x)\n",
    "        x = scatter_mean(updated_features, col, dim=0, dim_size=x.size(0))\n",
    "        return x + residual_features\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, atom_in_features, edge_attr_dim, preprocess_hidden_features, cheb_hidden_features, K, cheb_normalizations, dropout_rates, activation_fns, use_batch_norm, postprocess_hidden_features, out_features):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.atom_preprocess = nn.ModuleList([AtomEdgeInteraction(atom_in_features, edge_attr_dim, preprocess_hidden_features[0], dropout_rate=dropout_rates[0], use_batch_norm=use_batch_norm[0])])\n",
    "        for i in range(1, len(preprocess_hidden_features)):\n",
    "            layer = nn.Sequential(\n",
    "                KANLinear(preprocess_hidden_features[i-1], preprocess_hidden_features[i]),\n",
    "                nn.BatchNorm1d(preprocess_hidden_features[i]) if use_batch_norm[i] else nn.Identity(),\n",
    "                activation_fns[i](),\n",
    "                nn.Dropout(dropout_rates[i])\n",
    "            )\n",
    "            self.atom_preprocess.append(layer)\n",
    "\n",
    "        self.cheb_convolutions = nn.ModuleList()\n",
    "        in_channels = preprocess_hidden_features[-1]\n",
    "        for i in range(len(cheb_hidden_features)):\n",
    "            self.cheb_convolutions.append(ChebConv(in_channels, cheb_hidden_features[i], K[i], normalization=cheb_normalizations[i]))\n",
    "            in_channels = cheb_hidden_features[i]\n",
    "\n",
    "        self.postprocess = nn.ModuleList()\n",
    "        for i in range(len(postprocess_hidden_features)):\n",
    "            layer = nn.Sequential(\n",
    "                KANLinear(cheb_hidden_features[i-1] if i > 0 else cheb_hidden_features[-1], postprocess_hidden_features[i]),\n",
    "                nn.BatchNorm1d(postprocess_hidden_features[i]) if use_batch_norm[len(preprocess_hidden_features) + i] else nn.Identity(),\n",
    "                activation_fns[len(preprocess_hidden_features) + i](),\n",
    "                nn.Dropout(dropout_rates[len(preprocess_hidden_features) + i])\n",
    "            )\n",
    "            self.postprocess.append(layer)\n",
    "\n",
    "        self.output_layer = KANLinear(postprocess_hidden_features[-1], out_features)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        x = self.atom_preprocess[0](x, edge_index, edge_attr)\n",
    "        for layer in self.atom_preprocess[1:]:\n",
    "            x = layer(x)\n",
    "\n",
    "        for conv in self.cheb_convolutions:\n",
    "            x = F.relu(conv(x, edge_index))\n",
    "\n",
    "        for layer in self.postprocess:\n",
    "            x = layer(x)\n",
    "\n",
    "        return self.output_layer(x).squeeze(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded for finetuning.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from lion_pytorch import Lion\n",
    "from utils.utils import MoleculeDataModule, evaluate_model\n",
    "from utils.train import MoleculeModel\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_scatter import scatter_mean\n",
    "import torch.nn.functional as F\n",
    "\n",
    "in_features = 133\n",
    "out_features = 1\n",
    "edge_attr_dim = 14\n",
    "\n",
    "batch_size = 1024\n",
    "num_workers = 8\n",
    "\n",
    "preprocess_hidden_features = [128] * 9\n",
    "postprocess_hidden_features = [128, 128]\n",
    "cheb_hidden_features = [128, 128]\n",
    "K = [10, 16]\n",
    "cheb_normalization = ['sym', 'sym']\n",
    "\n",
    "dropout_rates = [0.0] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "activation_fns = [nn.PReLU] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "use_batch_norm = [True] * (len(preprocess_hidden_features) + len(postprocess_hidden_features))\n",
    "\n",
    "learning_rate = 2.2e-5\n",
    "weight_decay = 3e-5\n",
    "step_size = 30\n",
    "gamma = 0.2\n",
    "metric = 'rmse'\n",
    "\n",
    "# Определение архитектуры модели\n",
    "backbone = Model(\n",
    "    atom_in_features=in_features,\n",
    "    edge_attr_dim=edge_attr_dim,\n",
    "    preprocess_hidden_features=preprocess_hidden_features,\n",
    "    cheb_hidden_features=cheb_hidden_features,\n",
    "    K=K,\n",
    "    cheb_normalizations=cheb_normalization,\n",
    "    dropout_rates=dropout_rates,\n",
    "    activation_fns=activation_fns,\n",
    "    use_batch_norm=use_batch_norm,\n",
    "    postprocess_hidden_features=postprocess_hidden_features,\n",
    "    out_features=out_features\n",
    ")\n",
    "\n",
    "# Инициализация модели\n",
    "model = MoleculeModel(\n",
    "    model_backbone=backbone,\n",
    "    optimizer_class=Lion,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    step_size=step_size,\n",
    "    gamma=gamma,\n",
    "    batch_size=batch_size,\n",
    "    metric=metric\n",
    ")\n",
    "\n",
    "# Загрузка контрольной точки\n",
    "checkpoint_path = 'final_model.ckpt'\n",
    "model = MoleculeModel.load_from_checkpoint(checkpoint_path, model_backbone=backbone)\n",
    "\n",
    "print(\"Model loaded for finetuning.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024 \n",
    "num_workers = 8  \n",
    "dataset = torch.load(f'../data/QM_137k.pt')\n",
    "data_module = MoleculeDataModule(dataset, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolenko/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Evaluating: 100%|██████████| 134/134 [00:26<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RMSE: 0.0188\n",
      "Total R²: 0.9919\n"
     ]
    }
   ],
   "source": [
    "evaluate_model_full(model, dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type  | Params\n",
      "-----------------------------------------\n",
      "0 | model_backbone | Model | 2.3 M \n",
      "-----------------------------------------\n",
      "329 K     Trainable params\n",
      "1.9 M     Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.096     Total estimated model params size (MB)\n",
      "Metric val_loss improved. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.065\n",
      "Monitored metric val_loss did not improve in the last 5 records. Best score: 0.065. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее время обучения: 0:19:18\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from pytorch_lightning import Trainer, callbacks\n",
    "\n",
    "high_quality_dataset = torch.load(f'../data/QM_cool.pt')\n",
    "\n",
    "new_data_weight = 10\n",
    "old_data_weight = 1\n",
    "\n",
    "combined_dataset = dataset + high_quality_dataset\n",
    "weights = [old_data_weight] * len(dataset) + [new_data_weight] * len(high_quality_dataset)\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(combined_dataset), replacement=True)\n",
    "\n",
    "data_module = MoleculeDataModule(dataset=combined_dataset, batch_size=1024, num_workers=8)\n",
    "\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if \"postprocess\" not in name and \"output_layer\" not in name:\n",
    "        param.requires_grad = False\n",
    "\n",
    "optimizer = Lion(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "model.configure_optimizers = lambda: optimizer\n",
    "\n",
    "\n",
    "early_stop_callback = callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=True, mode='min')\n",
    "timer = callbacks.Timer()\n",
    "logger = pl.loggers.TensorBoardLogger('tb_logs', name='KAN_fine_loss')\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[early_stop_callback, timer],\n",
    "    enable_progress_bar=False,\n",
    "    logger=logger,\n",
    "    accelerator='gpu',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "seconds = timer.time_elapsed()\n",
    "h, m, s = int(seconds // 3600), int((seconds % 3600) // 60), int(seconds % 60)\n",
    "\n",
    "print(f\"Общее время обучения: {h}:{m:02d}:{s:02d}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolenko/.local/lib/python3.10/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Evaluating:  32%|███▏      | 43/134 [08:26<19:11, 12.65s/it]"
     ]
    }
   ],
   "source": [
    "evaluate_model_full(model, dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'evaluate_model_full' from 'utils.utils' (/home/nikolenko/work/gat/utils/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EarlyStopping, Timer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlion_pytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Lion\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MoleculeDataModule, evaluate_model_full\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MoleculeModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'evaluate_model_full' from 'utils.utils' (/home/nikolenko/work/gat/utils/utils.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import WeightedRandomSampler, DataLoader\n",
    "from pytorch_lightning import Trainer, callbacks\n",
    "from pytorch_lightning.callbacks import EarlyStopping, Timer\n",
    "from lion_pytorch import Lion\n",
    "from utils.utils import MoleculeDataModule\n",
    "from utils.train import MoleculeModel\n",
    "from torch import nn\n",
    "\n",
    "# Загружаем датасеты\n",
    "dataset = torch.load(f'../data/QM_137k.pt')\n",
    "high_quality_dataset = torch.load(f'../data/QM_cool.pt')\n",
    "\n",
    "# Комбинируем датасеты\n",
    "combined_dataset = dataset + high_quality_dataset\n",
    "\n",
    "# Определяем веса\n",
    "new_data_weight = 10\n",
    "old_data_weight = 1\n",
    "weights = [old_data_weight] * len(dataset) + [new_data_weight] * len(high_quality_dataset)\n",
    "\n",
    "# Создаем WeightedRandomSampler\n",
    "sampler = WeightedRandomSampler(weights, num_samples=len(combined_dataset), replacement=True)\n",
    "\n",
    "# Настраиваем DataModule с новым датасетом и семплером\n",
    "data_module = MoleculeDataModule(dataset=combined_dataset, batch_size=1024, num_workers=8, sampler=sampler)\n",
    "\n",
    "# Функция для постепенной разморозки слоев\n",
    "def unfreeze_layers(model, num_layers_to_unfreeze):\n",
    "    total_layers = len(list(model.parameters()))\n",
    "    for i, param in enumerate(model.parameters()):\n",
    "        if i >= total_layers - num_layers_to_unfreeze:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Функция для дообучения модели\n",
    "def finetune_model(model, method_name, num_layers_to_unfreeze=None, additional_layers=None):\n",
    "    if method_name == \"gradual_unfreeze\":\n",
    "        unfreeze_layers(model, num_layers_to_unfreeze)\n",
    "    elif method_name == \"additional_layers\":\n",
    "        if additional_layers is not None:\n",
    "            model.model_backbone.output_layer = nn.Sequential(\n",
    "                model.model_backbone.output_layer,\n",
    "                nn.ReLU(),\n",
    "                *additional_layers\n",
    "            )\n",
    "    \n",
    "    optimizer = Lion(filter(lambda p: p.requires_grad, model.parameters()), lr=2.2e-5, weight_decay=3e-5)\n",
    "    model.configure_optimizers = lambda: optimizer\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor='val_loss', patience=5, verbose=True, mode='min')\n",
    "    timer = Timer()\n",
    "    logger = pl.loggers.TensorBoardLogger('tb_logs', name=f'{method_name}_finetuning')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        max_epochs=100,\n",
    "        enable_checkpointing=False,\n",
    "        callbacks=[early_stop_callback, timer],\n",
    "        enable_progress_bar=True,\n",
    "        logger=logger,\n",
    "        accelerator='gpu',\n",
    "        devices=1\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, data_module)\n",
    "\n",
    "    seconds = timer.time_elapsed()\n",
    "    h, m, s = int(seconds // 3600), int((seconds % 3600) // 60), int(seconds % 60)\n",
    "    print(f\"Общее время дообучения ({method_name}): {h}:{m:02d}:{s:02d}\")\n",
    "\n",
    "    val_loss = evaluate_model_full(model, dataset, 1024, num_workers)\n",
    "    return val_loss\n",
    "\n",
    "# Загружаем модель из контрольной точки\n",
    "checkpoint_path = 'final_model.ckpt'\n",
    "model = MoleculeModel.load_from_checkpoint(checkpoint_path, model_backbone=backbone)\n",
    "\n",
    "# Применяем метод постепенной разморозки слоев\n",
    "model_copy = model.clone()\n",
    "val_loss_gradual_unfreeze = finetune_model(model_copy, \"gradual_unfreeze\", num_layers_to_unfreeze=2)\n",
    "\n",
    "# Применяем метод добавления новых слоев\n",
    "model_copy = model.clone()\n",
    "additional_layers = [nn.Linear(1, 1)]\n",
    "val_loss_additional_layers = finetune_model(model_copy, \"additional_layers\", additional_layers=additional_layers)\n",
    "\n",
    "# Применяем метод transfer learning\n",
    "model_copy = MoleculeModel(model_backbone=backbone)\n",
    "optimizer = Lion(model_copy.parameters(), lr=2.2e-5, weight_decay=3e-5)\n",
    "model_copy.configure_optimizers = lambda: optimizer\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', patience=5, verbose=True, mode='min')\n",
    "timer = Timer()\n",
    "logger = pl.loggers.TensorBoardLogger('tb_logs', name='transfer_learning')\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=100,\n",
    "    enable_checkpointing=False,\n",
    "    callbacks=[early_stop_callback, timer],\n",
    "    enable_progress_bar=True,\n",
    "    logger=logger,\n",
    "    accelerator='gpu',\n",
    "    devices=1\n",
    ")\n",
    "\n",
    "data_module = MoleculeDataModule(dataset=high_quality_dataset, batch_size=1024, num_workers=8)\n",
    "trainer.fit(model_copy, data_module)\n",
    "\n",
    "seconds = timer.time_elapsed()\n",
    "h, m, s = int(seconds // 3600), int((seconds % 3600) // 60), int(seconds % 60)\n",
    "print(f\"Общее время дообучения (transfer_learning): {h}:{m:02d}:{s:02d}\")\n",
    "\n",
    "val_loss_transfer_learning = evaluate_model_full(model_copy, dataset, 1024)\n",
    "\n",
    "# Выводим результаты\n",
    "print(f\"Результаты дообучения:\")\n",
    "print(f\"Постепенная разморозка слоев: Валид. ошибка (RMSE) = {val_loss_gradual_unfreeze:.4f}\")\n",
    "print(f\"Добавление новых слоев: Валид. ошибка (RMSE) = {val_loss_additional_layers:.4f}\")\n",
    "print(f\"Transfer Learning: Валид. ошибка (RMSE) = {val_loss_transfer_learning:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_geom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
